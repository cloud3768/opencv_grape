{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9ff3d44b-1499-4d84-a39d-1e36e87767ed",
   "metadata": {},
   "source": [
    "# yolov5葡萄数量预测模型训练"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fe45236e-7694-442e-bd2b-b8cdc1649c91",
   "metadata": {},
   "source": [
    "### 拉取代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ee65f97-7ba0-4d95-8c04-6b6dd65c30ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e:\\SRP\\grape_yolov5\\yolov5\n"
     ]
    }
   ],
   "source": [
    "%cd yolov5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b8625007-7a92-45b8-875e-893b16f4ad25",
   "metadata": {},
   "source": [
    "### 配置环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f359215-0270-4e9b-bef9-fe0ad8181efe",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8bf75dc9-7bca-46b9-9af8-744918aa70b6",
   "metadata": {},
   "source": [
    "在训练过程中使用的数据增强策略可以在使用的训练配置文件中设置，我们使用的是yolov7/data/hyp.scratch.tiny.yaml："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7dbd63-f57b-4d97-afa1-395f50e1800e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pycat data/hyps/hyp.scratch-high.yaml"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "27fd8432-33e3-451e-a4a6-3a1c33180213",
   "metadata": {},
   "source": [
    "## 训练模型"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b00e271c-9ec0-4d29-971f-29f2620423ea",
   "metadata": {},
   "source": [
    "### check anchor\n",
    "重新计算anchor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b3e855",
   "metadata": {},
   "source": [
    "葡萄数目预测anchor计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c202bea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Scanning ..\\datasets\\train.cache... 774 images, 0 backgrounds, 0 corrupt: 100%|██████████| 774/774 00:00\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mRunning kmeans for 6 anchors on 28400 points...\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.8733: 100%|██████████| 1000/1000 00:02\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mthr=0.20: 1.0000 best possible recall, 4.91 anchors past thr\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mn=6, img_size=640, metric_all=0.574/0.873-mean/best, past_thr=0.677-mean: 23,24, 32,32, 39,40, 46,47, 58,59, 275,350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[     22.876      23.957]\n",
      " [     31.837      32.303]\n",
      " [     39.005      39.773]\n",
      " [     46.383      47.215]\n",
      " [     58.382      58.948]\n",
      " [     275.14       350.2]]\n"
     ]
    }
   ],
   "source": [
    "import utils.autoanchor as autoAC\n",
    " \n",
    "# 对数据集重新计算 anchors\n",
    "new_anchors = autoAC.kmean_anchors('./data/grape_number_prediction.yaml', 6, 480, 5.0, 1000, False)\n",
    "print(new_anchors)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0cfc080f-cffc-44ad-a199-fa7fc2502cc4",
   "metadata": {},
   "source": [
    "### 查看模型配置文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a1e079-1bd6-48c4-8cfb-25c73327c519",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pycat models/yolov5s.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8ca16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python models/yolo.py --cfg yolov5s.yaml"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6c5b6fd1-8f23-4738-a9e2-ecad8ff7d536",
   "metadata": {},
   "source": [
    "### 训练模型"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8432f596-6a4c-4322-8286-47d719ff0581",
   "metadata": {},
   "source": [
    "葡萄串检测模型训练："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94271fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train.py --workers 12 --batch-size 16 --cfg models/yolov5s_grape.yaml --epochs 10 --device 0 --weights yolov5s.pt --hyp data/hyps/hyp.scratch-low.yaml --data data/grape.yaml --img 640"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954d3370",
   "metadata": {},
   "source": [
    "葡萄数目检测模型训练："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d23d7a-1e1e-40c3-9074-b16e3241a22f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python train.py --workers 12 --batch-size 4 --cfg models/yolov5s_num_predection.yaml --epochs 500 --device 0 --weights yolov5s.pt --hyp data/hyps/hyp.scratch-high.yaml --data data/grape_number_prediction.yaml --img 640"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f1854890-0249-49d9-a249-4a92989e3c99",
   "metadata": {},
   "source": [
    "## 查看结果"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ff417af8-f318-46a3-a616-e98dcc70021d",
   "metadata": {},
   "source": [
    "训练结束，可以看到run/train文件夹下保存了训练结果，加载best.pt查看推理结果："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca15c0b",
   "metadata": {},
   "source": [
    "### 图片灰度处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74fa317c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# 打开图像文件\n",
    "image = Image.open('../datasets/grape/1970_01_01_18_40_IMG_0688.JPG')\n",
    "\n",
    "# 将图像转换为灰度图像\n",
    "gray_image = image.convert('L')\n",
    "\n",
    "# 保存灰度图像\n",
    "gray_image.save('../1970_01_01_18_40_IMG_0688.JPG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cc8687",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python detect.py --weights best.pt --source ../datasets/images/test --img-size 640 --device 0 --data yolov5/data/grape.yaml --save-txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9412f72f-dbc4-4e33-a466-dd8afbc3f931",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['yolov5s_MobileNetv3_small_int8.onnx'], source=detect_image, data=data\\coco128.yaml, imgsz=[640, 640], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=0, view_img=False, save_txt=True, save_conf=True, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs\\detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
      "YOLOv5  2024-2-27 Python-3.9.13 torch-2.1.2+cu121 CUDA:0 (NVIDIA GeForce RTX 4060 Laptop GPU, 8188MiB)\n",
      "\n",
      "Loading yolov5s_MobileNetv3_small_int8.onnx for ONNX Runtime inference...\n",
      "image 1/4 E:\\SRP\\grape_yolov5\\yolov5\\detect_image\\P0040.jpg: 640x640 (no detections), 78.8ms\n",
      "image 2/4 E:\\SRP\\grape_yolov5\\yolov5\\detect_image\\P0041.jpg: 640x640 (no detections), 52.0ms\n",
      "image 3/4 E:\\SRP\\grape_yolov5\\yolov5\\detect_image\\TEST_1.jpg: 640x640 (no detections), 47.4ms\n",
      "image 4/4 E:\\SRP\\grape_yolov5\\yolov5\\detect_image\\TEST_2.jpg: 640x640 (no detections), 50.5ms\n",
      "Speed: 18.8ms pre-process, 57.2ms inference, 5.2ms NMS per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\exp2\u001b[0m\n",
      "0 labels saved to runs\\detect\\exp2\\labels\n"
     ]
    }
   ],
   "source": [
    "!python detect.py --weights ShuffleNet_Yolov5_int8.onnx --source detect_image --img-size 640 --device 0  --save-txt  --save-conf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "84ec72df-73fb-47fe-b143-58dd5a32f983",
   "metadata": {},
   "source": [
    "可以看到结果保存在runs/detect/exp?/*.jpg，查看结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e91b08-7cb3-42cb-8f31-f33a8d17275d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from  matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "img = cv2.imread(\"runs/detect/exp5/G0006.jpg\")\n",
    "plt.imshow(img[:,:,::-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfff4567",
   "metadata": {},
   "source": [
    "## 测试训练的权重"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0716ce8",
   "metadata": {},
   "source": [
    "### 正常测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b078dd61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mdata=data/grape_number_prediction.yaml, weights=['best.pt'], batch_size=16, imgsz=480, conf_thres=0.001, iou_thres=0.6, max_det=300, task=val, device=cpu, workers=12, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project=runs\\val, name=exp, exist_ok=False, half=False, dnn=False\n",
      "YOLOv5  2024-2-27 Python-3.9.13 torch-2.1.2+cu121 CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s_tiny summary: 169 layers, 150126 parameters, 0 gradients, 1.0 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning E:\\SRP\\grape_yolov5\\datasets\\val.cache... 43 images, 0 backgrounds, 0 corrupt: 100%|██████████| 43/43 00:00\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning E:\\SRP\\grape_yolov5\\datasets\\val.cache... 43 images, 0 backgrounds, 0 corrupt: 100%|██████████| 43/43 00:00\n",
      "\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/3 00:00\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95:  33%|███▎      | 1/3 00:16\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95:  67%|██████▋   | 2/3 00:20\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 3/3 00:23\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 3/3 00:23\n",
      "                   all         43       1507      0.722        0.8      0.818      0.469\n",
      "                 grape         43         43      0.716      0.721      0.748      0.327\n",
      "                 berry         43       1464      0.727       0.88      0.888      0.611\n",
      "Speed: 3.0ms pre-process, 113.0ms inference, 20.6ms NMS per image at shape (16, 3, 480, 480)\n",
      "Results saved to \u001b[1mruns\\val\\exp9\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python val.py --workers 12 --weights best.pt --batch-size 16 --device cpu --data data/grape_number_prediction.yaml --img 480"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c5bef4",
   "metadata": {},
   "source": [
    "### 模型剪枝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c33c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.torch_utils import prune\n",
    "\n",
    "prune('best.pt', 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6000a39d",
   "metadata": {},
   "source": [
    "## 导出权重文件为其他格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "259f757b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mexport: \u001b[0mdata=E:\\SRP\\grape_yolov5\\yolov5\\data\\coco128.yaml, weights=['./best.pt'], imgsz=[640, 640], batch_size=1, device=cpu, half=False, inplace=False, keras=False, optimize=True, int8=False, per_tensor=False, dynamic=False, simplify=False, opset=17, verbose=False, workspace=4, nms=False, agnostic_nms=False, topk_per_class=100, topk_all=100, iou_thres=0.45, conf_thres=0.25, include=['onnx']\n",
      "YOLOv5  2024-2-27 Python-3.9.13 torch-2.1.2+cu121 CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s_MobileNetv3_small summary: 286 layers, 2562853 parameters, 0 gradients, 4.4 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from best.pt with output shape (1, 25200, 7) (5.3 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.14.1...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success  2.2s, saved as best.onnx (10.2 MB)\n",
      "\n",
      "Export complete (2.6s)\n",
      "Results saved to \u001b[1mE:\\SRP\\grape_yolov5\\yolov5\u001b[0m\n",
      "Detect:          python detect.py --weights best.onnx \n",
      "Validate:        python val.py --weights best.onnx \n",
      "PyTorch Hub:     model = torch.hub.load('ultralytics/yolov5', 'custom', 'best.onnx')  \n",
      "Visualize:       https://netron.app\n"
     ]
    }
   ],
   "source": [
    "!python export.py --weights ./best.pt --include onnx --device cpu --optimize"
   ]
  }
 ],
 "metadata": {
  "AIGalleryInfo": "",
  "flavorInfo": {
   "architecture": "X86_64",
   "category": "GPU"
  },
  "imageInfo": {
   "id": "278e88d1-5b71-4766-8502-b3ba72e824d9",
   "name": "pytorch1.8-cuda10.2-cudnn7-ubuntu18.04"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
